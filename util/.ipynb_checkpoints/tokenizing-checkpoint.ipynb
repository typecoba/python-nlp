{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.46'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soynlp\n",
    "soynlp.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent 0: 추천인아이디요. . 추천인정보에 인스타아이디를 넣는건가요 아니면 로그인할때 메일주소를 넣는 ...\n",
      "sent 1: 포스트 금액 문의 안녕하세요. 지금 신청은 했는데요. 제가 처음이라.. 잘 몰라서요. 포스 ...\n",
      "sent 2: 제가 페이스북 페이지가 있는데 어떻게 등록하나요? 알려주세요 \n",
      "sent 3: 페이지관리자입니다. 팔로우 1048명 페이지관리자인데 등록하고싶은데 등록이 안되서요... \n"
     ]
    }
   ],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "corpus_fname = '../data/md_contactus_question_spacing.txt'\n",
    "corpus = DoublespaceLineCorpus(corpus_fname, iter_sent=True, num_sent=4)\n",
    "\n",
    "for n_sent, sent in enumerate(corpus):\n",
    "    print('sent %d: %s %s'% (n_sent, sent[:50], '' if len(sent) < 50 else '...'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs 0 (#sents= 1): 추천인아이디요. . 추천인정보에 인스타아이디를 넣는건가요 아니면 로그인할때 메일주소를 넣는 ...\n",
      "docs 1 (#sents= 1): 포스트 금액 문의 안녕하세요. 지금 신청은 했는데요. 제가 처음이라.. 잘 몰라서요. 포스 ...\n",
      "docs 2 (#sents= 1): 제가 페이스북 페이지가 있는데 어떻게 등록하나요? 알려주세요 \n"
     ]
    }
   ],
   "source": [
    "corpus = DoublespaceLineCorpus(corpus_fname, iter_sent=False, num_doc=3)\n",
    "\n",
    "for n_doc, doc in enumerate(corpus):\n",
    "    print('docs %d (#sents= %d): %s %s'% (n_doc, len(doc.split('  ')), doc[:50].strip(), '' if len(doc) < 50 else '...'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordExtractor (Cohesion score, Branching Entropy, Accessor Variety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.231 Gb\n"
     ]
    }
   ],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "\n",
    "corpus = DoublespaceLineCorpus(corpus_fname, iter_sent=True)\n",
    "word_extractor = WordExtractor(left_max_length=10,\n",
    "                               right_max_length=6,\n",
    "                               min_count=100)\n",
    "word_extractor.train(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " cohesion probabilities ... (1 in 550)\r",
      "all cohesion probabilities was computed. # words = 348\n",
      "\r",
      "all branching entropies was computed # words = 12214\n",
      "\r",
      "all accessor variety was computed # words = 12214\n"
     ]
    }
   ],
   "source": [
    "words = word_extractor.word_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scores(cohesion_forward=0.9766990291262136, cohesion_backward=0, left_branching_entropy=3.1200165166238483, right_branching_entropy=3.176185129512808, left_accessor_variety=45, right_accessor_variety=34, leftside_frequency=503, rightside_frequency=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words['환급']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766990291262136"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words['환급'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "환급 (3.044, 2.795)\n",
      "환급금 (0.000, 0.000)\n"
     ]
    }
   ],
   "source": [
    "for word in ['환급', '환급금']:\n",
    "    print(word, '(%.3f, %.3f)' % (scores[word].left_branching_entropy,\n",
    "                                  scores[word].right_branching_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어   (빈도수, cohesion, branching entropy)\n",
      "\n",
      "ㅠㅠ\t\t\t(528, 0.887, 4.111)\n",
      "했는데\t\t\t(244, 0.774, 4.029)\n",
      "^^\t\t\t(104, 0.945, 3.816)\n",
      "혹시\t\t\t(273, 0.945, 3.721)\n",
      "ㅜㅜ\t\t\t(132, 0.857, 3.663)\n",
      "합니다\t\t\t(120, 0.943, 3.538)\n",
      "캠페인\t\t\t(1266, 0.981, 3.337)\n",
      "제가\t\t\t(524, 0.378, 4.257)\n",
      "드립니다.\t\t\t(155, 0.764, 3.470)\n",
      "포스팅\t\t\t(840, 0.696, 3.556)\n",
      "환급\t\t\t(503, 0.977, 3.176)\n",
      "미디언스\t\t\t(218, 0.841, 3.153)\n",
      "감사합니다.\t\t\t(134, 0.835, 3.059)\n",
      "드립니다\t\t\t(238, 0.806, 3.073)\n",
      "언제\t\t\t(377, 0.969, 2.862)\n",
      "영상\t\t\t(161, 0.834, 3.008)\n",
      "변경\t\t\t(326, 0.959, 2.830)\n",
      "따로\t\t\t(127, 0.864, 2.890)\n",
      "안녕하세요.\t\t\t(183, 0.586, 3.275)\n",
      "안녕하세요\t\t\t(821, 0.746, 3.016)\n",
      "업로드\t\t\t(322, 0.871, 2.767)\n",
      "부탁드려요\t\t\t(145, 0.654, 3.019)\n",
      "취소\t\t\t(652, 0.991, 2.592)\n",
      "오늘\t\t\t(331, 0.415, 3.413)\n",
      "되나요?\t\t\t(104, 0.459, 3.309)\n",
      "승인\t\t\t(407, 0.998, 2.465)\n",
      "잘못\t\t\t(302, 0.904, 2.555)\n",
      "관련\t\t\t(349, 0.864, 2.556)\n",
      "하는데\t\t\t(163, 0.354, 3.436)\n",
      "채널\t\t\t(212, 0.951, 2.414)\n",
      "광고\t\t\t(161, 0.976, 2.326)\n",
      "삭제\t\t\t(158, 1.000, 2.300)\n",
      "문의\t\t\t(1368, 0.755, 2.578)\n",
      "등록\t\t\t(717, 0.920, 2.334)\n",
      "완료\t\t\t(164, 0.970, 2.275)\n",
      "수정\t\t\t(451, 0.614, 2.715)\n",
      "있는데\t\t\t(122, 0.328, 3.338)\n",
      "요청\t\t\t(176, 0.779, 2.465)\n",
      "진행\t\t\t(648, 0.969, 2.230)\n",
      "다시\t\t\t(380, 0.445, 2.994)\n",
      "신청\t\t\t(904, 0.763, 2.446)\n",
      "방문\t\t\t(274, 0.573, 2.728)\n",
      "팔로워\t\t\t(200, 0.880, 2.293)\n",
      "해서\t\t\t(116, 0.146, 4.081)\n",
      "부탁드립니다\t\t\t(308, 0.828, 2.346)\n",
      "입니다\t\t\t(137, 0.463, 2.897)\n",
      "문의드립니다.\t\t\t(121, 0.637, 2.546)\n",
      "V포인트\t\t\t(110, 0.980, 2.114)\n",
      "포스트\t\t\t(505, 0.540, 2.692)\n",
      "아이디\t\t\t(272, 0.442, 2.885)\n",
      "인스타그램\t\t\t(240, 0.693, 2.431)\n",
      "피드\t\t\t(113, 0.693, 2.353)\n",
      "문의드려요\t\t\t(144, 0.531, 2.600)\n",
      "가이드\t\t\t(235, 0.519, 2.613)\n",
      "죄송합니다\t\t\t(152, 0.892, 2.049)\n",
      "사진\t\t\t(243, 0.416, 2.804)\n",
      "선정\t\t\t(314, 0.749, 2.193)\n",
      "메일\t\t\t(193, 0.675, 2.288)\n",
      "참여\t\t\t(190, 0.757, 2.171)\n",
      "탈퇴\t\t\t(169, 0.966, 1.904)\n",
      "부탁드립니다.\t\t\t(120, 0.730, 2.174)\n",
      "캐시인\t\t\t(373, 0.888, 1.968)\n",
      "계정\t\t\t(278, 0.451, 2.632)\n",
      "포인트\t\t\t(334, 0.439, 2.649)\n",
      "동영상\t\t\t(130, 0.725, 2.122)\n",
      "어떻게\t\t\t(555, 0.656, 2.213)\n",
      "마케팅\t\t\t(172, 0.596, 2.275)\n",
      "아직\t\t\t(433, 0.312, 2.903)\n",
      "오류\t\t\t(216, 0.271, 3.000)\n",
      "배송\t\t\t(502, 0.964, 1.718)\n",
      "지금\t\t\t(172, 0.196, 3.294)\n",
      "인플루언서\t\t\t(156, 0.622, 2.076)\n",
      "확인\t\t\t(755, 0.965, 1.628)\n",
      "리그램\t\t\t(109, 0.623, 2.008)\n",
      "계속\t\t\t(127, 0.206, 3.102)\n",
      "내용\t\t\t(146, 0.438, 2.343)\n",
      "다른\t\t\t(210, 0.246, 2.902)\n",
      "카드\t\t\t(113, 0.551, 2.009)\n",
      "제품\t\t\t(546, 0.394, 2.257)\n",
      "하나요\t\t\t(114, 0.296, 2.511)\n",
      "문의드립니다\t\t\t(300, 0.698, 1.646)\n",
      "아디다스\t\t\t(109, 0.428, 2.062)\n",
      "주소\t\t\t(246, 0.436, 1.997)\n",
      "답변\t\t\t(156, 0.862, 1.315)\n",
      "있는\t\t\t(358, 0.317, 2.297)\n",
      "배송이\t\t\t(126, 0.492, 1.805)\n",
      "블라인드\t\t\t(136, 0.909, 1.189)\n",
      "인스타\t\t\t(591, 0.753, 1.368)\n",
      "되는\t\t\t(252, 0.235, 2.525)\n",
      "리뷰\t\t\t(106, 0.377, 1.999)\n",
      "하는\t\t\t(318, 0.245, 2.414)\n",
      "가능한\t\t\t(115, 0.363, 1.908)\n",
      "연락\t\t\t(243, 0.503, 1.564)\n",
      "입금\t\t\t(358, 0.560, 1.427)\n",
      "감사합니다\t\t\t(229, 0.913, 0.915)\n",
      "상품\t\t\t(108, 0.437, 1.624)\n",
      "통장\t\t\t(148, 0.708, 1.109)\n",
      "제품이\t\t\t(130, 0.306, 1.934)\n",
      "방법\t\t\t(114, 0.238, 2.170)\n",
      "지급\t\t\t(176, 0.201, 2.272)\n"
     ]
    }
   ],
   "source": [
    "def word_score(score):\n",
    "    import math\n",
    "    return (score.cohesion_forward * math.exp(score.right_branching_entropy))\n",
    "\n",
    "print('단어   (빈도수, cohesion, branching entropy)\\n')\n",
    "for word, score in sorted(words.items(), key=lambda x:word_score(x[1]), reverse=True)[:100]:\n",
    "    print('%s\\t\\t\\t(%d, %.3f, %.3f)' % (word, \n",
    "                                   score.leftside_frequency, \n",
    "                                   score.cohesion_forward,\n",
    "                                   score.right_branching_entropy\n",
    "                                  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used default noun predictor; Sejong corpus based logistic predictor\n",
      "C:/Users/mediance_ssh/Anaconda3/lib/site-packages/soynlp\n",
      "scan vocabulary ... \n",
      "done (Lset, Rset, Eojeol) = (60666, 39366, 25849)\n",
      "predicting noun score was done                                        \n",
      "before postprocessing 8736\n",
      "_noun_scores_ 1649\n",
      "checking hardrules ... done\n",
      "after postprocessing 1018\n",
      "extracted 25 compounds from eojeols"
     ]
    }
   ],
   "source": [
    "from soynlp.noun import LRNounExtractor\n",
    "from soynlp.noun import NewsNounExtractor\n",
    "\n",
    "corpus = DoublespaceLineCorpus(corpus_fname, iter_sent=True)\n",
    "noun_extractor = NewsNounExtractor()\n",
    "nouns = noun_extractor.train_extract(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "환급 is noun? True\n",
      "\n",
      "환급금 is noun? False\n",
      "\n",
      "모바일로 is noun? False\n",
      "\n",
      "캠페인을 is noun? False\n",
      "\n",
      "포스팅 is noun? True\n",
      "\n",
      "포스트 is noun? True\n",
      "\n",
      "캐시한급받고싶어요 is noun? False\n",
      "\n",
      "죄송하단 is noun? False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in ['환급', '환급금', '모바일로', '캠페인을', '포스팅', '포스트', '캐시한급받고싶어요', '죄송하단']:\n",
    "    print('%s is noun? %r\\n' % (word, word in nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.tokenizer import LTokenizer,MaxScoreTokenizer, RegexTokenizer\n",
    "cohesion_scores = {word:score.cohesion_forward for word, score in scores.items()}\n",
    "ltokenizer = LTokenizer(scores=cohesion_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['어느',\n",
       " '순간',\n",
       " '사진',\n",
       " '이',\n",
       " '삭제',\n",
       " '되있더라구요',\n",
       " '그래서',\n",
       " '재등록',\n",
       " '하긴했는데',\n",
       " '이럴땐',\n",
       " '어떡게해야하나여']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltokenizer.tokenize('어느순간 사진이 삭제되있더라구요 그래서 재등록하긴했는데 이럴땐어떡게해야하나여')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('어느', '순간'),\n",
       " ('사진', '이'),\n",
       " ('삭제', '되있더라구요'),\n",
       " ('그래서', ''),\n",
       " ('재등록', '하긴했는데'),\n",
       " ('이럴땐', '어떡게해야하나여')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltokenizer.tokenize('어느순간 사진이 삭제되있더라구요 그래서 재등록하긴했는데 이럴땐어떡게해야하나여', flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['어느', '사진', '삭제', '그래서', '재등록', '이럴땐']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltokenizer.tokenize('어느순간 사진이 삭제되있더라구요 그래서 재등록하긴했는데 이럴땐어떡게해야하나여', remove_r=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['어느',\n",
       " '순간',\n",
       " '사진',\n",
       " '이',\n",
       " '삭제',\n",
       " '되',\n",
       " '있더라구요',\n",
       " '그래서',\n",
       " '재',\n",
       " '등록',\n",
       " '하긴',\n",
       " '했는데',\n",
       " '이럴땐',\n",
       " '어떡',\n",
       " '게',\n",
       " '해야하나',\n",
       " '여']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxscoretokenizer = MaxScoreTokenizer(scores=cohesion_scores)\n",
    "maxscoretokenizer.tokenize('어느순간 사진이 삭제되있더라구요 그래서 재등록하긴했는데 이럴땐어떡게해야하나여'.replace(' ',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['추천인아이디요 . . 추천인정보에 인스타아이디를 넣는건가요 아니면 로그인할때 메일주소를 넣는건가요 ? 제가 소개해서 가입하신분들이 인스타아이디 dlsks gml 01 로 추천인을 넣으셨다는데 포인트정보가 없어서요',\n",
       " '포스트 금액 문의 안녕하세요 . 지금 신청은 했는데요 . 제가 처음이라 .. 잘 몰라서요 . 포스트 금액을 제가 설정하는 것이던데 , 제가 받고 싶은 금액을 적으면 되는것인지 .. 아님 당첨되면 회사에서 정한 금액을 받는 것인지 궁금해요 . 제가 금액을 높게 신청하면 컨텍이 안되는 것인지 .. 궁금하네요',\n",
       " '제가 페이스북 페이지가 있는데 어떻게 등록하나요 ? 알려주세요',\n",
       " '페이지관리자입니다 . 팔로우 1048 명 페이지관리자인데 등록하고싶은데 등록이 안되서요 ...',\n",
       " '페이지주소입니다 . https :// www . facebook . com / ukeukeuke 18',\n",
       " '정진우싱글앨범 문의요 싱금앨범을 보내주면 포스트하는거 아닌가요 ? 아니면 인스타에 올리 사진이나 동영상은 어디서받아야되나요 ? 메일로 보내주시는건가요 ?',\n",
       " '방금캠페인 포인트를 15 만포인트 신청햇는데 그럼 이거 포스팅해주면 몇 포인트가 들어온다는거죠 ?',\n",
       " '네 자세한 답변감사하구요 결론적으로 이번 캠페인은 입찰금액이 들어오는게 아닌 리워드로 받은것하고 1000 원 포인트만 들어온다는 이야기시죠 ? 처음이라서요',\n",
       " '제휴문의 인스타 시스템을 운영 하고 있습니다 .. 효과적인 아이디어가 있어 상호 윈윈 할만한 수익건이라 생각되어 연락 드립니다 .. 연락 한번 부탁 합니다 .. 010 4871 9487 이승우 대표 입니다',\n",
       " '제휴 관련 회신입니다 .. 회신 메일은 잘받았습니다 . 혹시 연락척 없던데 담당자분 연락처 없어서 혹시 빠지신게 아닌가 해서요 .. 자료는 보내 드릴수 있지만 좀 더 매력 있는 제안으로 협력 하고 싶어서 그렇습니다 ..']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "regextokenizer = RegexTokenizer() # regexTokenizer\n",
    "from soynlp.tokenizer import normalize # nomalize\n",
    "\n",
    "corpus_fname = '../data/md_contactus_question_spacing.txt'\n",
    "ary = []\n",
    "\n",
    "with open(corpus_fname, encoding='utf-8') as f:\n",
    "    for i,sent in enumerate(f):        \n",
    "        if (i==10): break        \n",
    "                \n",
    "        # regexTokenizer\n",
    "        text=''\n",
    "        for word in regextokenizer.tokenize(sent):            \n",
    "            text += word+' '\n",
    "        sent = text\n",
    "        \n",
    "        # 노멀라이징        \n",
    "        text=''\n",
    "        for word in normalize(sent, n_repeat=2):\n",
    "            text += word\n",
    "        sent = text\n",
    "        \n",
    "        # 토크나이징\n",
    "#         text=''\n",
    "#         for word in ltokenizer.tokenize(sent, remove_r=False):\n",
    "#         for word in maxscoretokenizer.tokenize(sent.replace(' ','')):\n",
    "#             text += word+' '        \n",
    "        \n",
    "        ary.append(text)\n",
    "\n",
    "ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writedata.py\n",
    "f = open(\"../data/md_contactus_question_tokenized.txt\", 'w', encoding='utf-8')\n",
    "for i,sent in enumerate(ary):\n",
    "    data = ary[i]+\"\\n\"\n",
    "    f.write(data)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['추천 인 아이디 요 . . 추천 인 정보 에 인스타 아이디 를 넣는 건 가요 아니 면 로그 인할 때 메일 주소 를 넣는 건 가요 ? \\xa0 제 가 소개해서 가입하신 분 들 이 인스타 아이디 dlsksgml 01 로 추천 인 을 넣으 셨 다는 데 포인트 정보 가 없어 서요 ',\n",
       " '추천 아이디 추천 정보 인스타 아이디 가요 로그 때 메일 주소 가요 제 분 인스타 아이디 로 추천 셨 포인트 정보 서요 ']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## twitter tokenizer\n",
    "corpus_fname = '../data/md_contactus_question_spacing.txt'\n",
    "ary = []\n",
    "\n",
    "with open(corpus_fname, encoding='utf-8') as f:\n",
    "    for i,sent in enumerate(f):\n",
    "        if(i==1): break\n",
    "        text = ''\n",
    "        for word in twitter.morphs(sent):\n",
    "            text += word+\" \"\n",
    "        ary.append(text)\n",
    "        text = ''\n",
    "        for word in twitter.nouns(sent):\n",
    "            text += word+\" \"\n",
    "        ary.append(text)\n",
    "\n",
    "ary        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## twitter tokenizer & word counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('문의', 1625), ('캠페인', 1324), ('포스팅', 893), ('제', 662), ('신청', 646), ('확인', 633), ('포인트', 609), ('제품', 591), ('등록', 584), ('취소', 566), ('포스트', 544), ('배송', 514), ('환급', 509), ('일', 505), ('수정', 481), ('아직', 432), ('승인', 432), ('캐시', 428), ('변경', 427), ('가요', 425)]\n"
     ]
    }
   ],
   "source": [
    "corpus_fname = '../data/md_contactus_question_spacing.txt'\n",
    "ary = []\n",
    "\n",
    "with open(corpus_fname, encoding='utf-8') as f:\n",
    "    for i,sent in enumerate(f):\n",
    "        ary.append(twitter.nouns(sent))\n",
    "\n",
    "flatten_docs = [word for sent in ary for word in sent]\n",
    "count = Counter(flatten_docs)\n",
    "print(count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 임베딩 후 시각화... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "    \n",
    "class CommentWord2Vec:\n",
    "    \n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "        if not os.path.exists(fname):\n",
    "            print('File not found: %s' % fname)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                movie_idx, text, score = doc.split('\\t')\n",
    "                yield text.split()\n",
    "                \n",
    "                \n",
    "word2vec_corpus = CommentWord2Vec(tokenized_corpus_fname)\n",
    "\n",
    "for num_doc, doc in enumerate(word2vec_corpus):\n",
    "    if num_doc > 5: break\n",
    "    print(doc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
