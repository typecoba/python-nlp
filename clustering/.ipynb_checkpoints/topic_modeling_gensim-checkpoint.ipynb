{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tag data가 topic 모델링 하기에 적합한 형태인건가?\n",
    "단문-갯수가 많은 데이터일때 부적합한점이라고 생각되는것은 없음...\n",
    "\n",
    "항상 처음 겪는 어려움이 학습되어진 데이터를 가져오는데 그게 어떻게 이루어져서 나온건지\n",
    "파악이 안됨... 한번 정리를 해놔야 함\n",
    "\n",
    "day6 파일을 기준으로 파악해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 벡터라이징을 한 뒤 term frequency matrix 만드는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_fname = '../NPL/data/corpus_10days/news/2016-10-24_article_all_normed.txt'\n",
    "dictionary_fname = './2016-10-24_article_all_normed_corpus.dictionary'\n",
    "ldamodel_fname = './2016-10-24_article_all_normed_corpus_lda.pkl'\n",
    "TRAIN_LDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ... \n",
      "\n",
      "의원 60명 내무장관에게 서한 철거 때 어린이 안전 신경 써야  보호자 없는 아동 난민 70명은 영국에 도착  22일 경찰에 돌 던지는 칼레 난민 연합뉴스  파리 런던 연합뉴스 박 ... \n",
      "\n",
      "경찰 로고 연합뉴스 캡처  해남 연합뉴스 박철홍 기자 23일 오후 10시 12분께 전남 해남군 해남읍의 박모 65 씨의 주택에서 불이 났다는 이웃 주민의 신고가 119 상황실에 접 ... \n",
      "\n",
      "상파울루 연합뉴스 김재순 통신원 브라질 경제의 침체 장기화에 따른 지방정부 재정난이 갈수록 확산하고 있다  23일 현지시간 브라질 도시협의회 에 따르면 연방정부를 통해 재정 현황을 ... \n",
      "\n",
      "로스앤젤레스 연합뉴스 장현구 특파원 미국 대통령 선거의 판세가 서서히 민주당 힐러리 클린턴 전 국무장관 쪽으로 기우는 상황에서 공화당 도널드 트럼프를 지지하는 메이저 신문이 처음으 ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 파일 열어 확인 \n",
    "with open(corpus_fname, encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f).strip()[:100], '... \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# doublespaceLIneCorpus는 문서형식을 정해서 편리하게 사용하기 위한 class..\n",
    "# 한줄이 하나의 doc이며 줄바꿈기호는 doublespace로 한다.. \n",
    "# iterator로 doc하나씩 출력함..\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "corpus_doc = DoublespaceLineCorpus(corpus_fname, iter_sent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "의원 60명 내무장관에게 서한 철거 때 어린이 안전 신경 써야  보호자 없는 아동 난민 70명은 영국에 도착  22일 경찰에 돌 던지는 칼레 난민 연합뉴스  파리 런던 연합뉴스 박성진 황정우 특파원 프랑스 칼레 난민촌 철거를 이틀 앞둔 22일 현지시간 난민촌 주변에서 현지 경찰과 난민이 충돌했다고 현지 프랑스앵포가 보도했다  난민촌 철거에 반대하는 난민 50명가량은 경찰을 향해 유리병과 돌을 던졌으며 경찰은 연막탄을 쏘면서 이들을 해산했다  이 과정에서 별다른 부상자는 나오지 않았다  프랑스 정부는 24일부터 약 일주일간에 걸쳐 불도저 등을 동원해 칼레 난민촌을 철거하고 이곳에 머무는 난민 6천여 명을 전국 난민 시설에 분산 수용할 계획이다  난민과 난민 지원 비정부기구가 철거 반대 운동을 벌일 수 있을 것으로 보고 프랑스 정부는 경찰을 증파해 현재 1천200여 명이 칼레에서 철거 작업에 대비하고 있다  또 칼레에서 가까운 벨기에도 난민들이 자국으로 넘어오지나 않을까 우려하며 자국과 프랑스 간 국경에 120명의 경찰을 배치했다  프랑스 칼레 난민촌의 모습 연합뉴스  프랑스 정부는 난민들에게 수천 장의 철거 안내서를 배부하고 난민촌 폐쇄 및 재배치 계획을 설명했다  영불 해협을 사이에 두고 영국과 마주 보는 프랑스 칼레에는 시리아 내전을 피하거나 중동 아프리카에서 건너온 난민이 몰려 살고 있다  이들은 상 하수 시설 화장실 등이 제대로 갖춰져 있지 않은 등 생활환경이 열악해 정글 로 불리는 난민촌에 머물며 상대적으로 일자리를 구하기 쉽고 영어를 사용하는 영국 밀입국을 시도하고 있다  한 프랑스 난민지원 단체는 2천 명가량의 난민이 여전히 영국행을 희망하며 난민촌에서 떠나길 거부하는 것으로 추산했다  프랑스 정부는 난민촌에서 떠나기를 거부하는 난민이 있다면 공권력을 행사할 수밖에 없다는 입장이다  세이브 더 칠드런 등 비정부기구와 영국 의원 60명은 난민촌 철거 시 어린이들이 안전할 수 있도록 프랑스 정부가 각별한 신경을 써 달라고 당부하는 서명이 담긴 서한을 베르나르 카즈뇌브 프랑스 내무장관에게 보냈다  카즈뇌브 장관은 난민촌이 철거된 뒤 난민 모두에게 온전한 쉼터를 제공하겠다 고 밝혔다  지난 3월 칼레 난민촌 철거 반대 시위를 벌이는 난민들 연합뉴스  한편 정글에서 지내던 아동 70여명이 철거를 앞두고 영국에 도착했거나 도착할 예정이라고 영국 언론들이 전했다  지난 22일 54명이 영국에 도착한 데 이어 23일 중 24명이 추가로 영국에 들어올 예정이다  이들 54명은 주로 아프리카 에리트레아 출신 여자 아동들로서 이른바 둡스 개정법 이 적용된 첫 사례다  독일 나치를 피해 영국에 온 난민 출신인 둡스 영국 상원의원의 이름을 딴 이 법은 보호자 없는 13세 미만 난민 아동의 수용 대상을 영국에 친척이 있지 경우로 확대했다  이전 관련 규정인 더블린 규정은 보호자 없는 난민 아동의 경우 영국에 이들을 돌봐줄 친척이 있는 경우에만 허용했다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(corpus_doc):\n",
    "    if i > 1 : break\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보상 0.6677393571428571\n",
      "차금주 0.5179402916666668\n",
      "데일리안 0.999867\n",
      "나가지 0.505755\n",
      "일각 0.9999048676470588\n",
      "롯데면세점 0.20054214427860698\n",
      "휴식 0.3009156557377049\n",
      "항목 0.7955528834951455\n",
      "인구 0.672447770700637\n",
      "제시 0.9887290680379747\n",
      "대통령 0.5903813165343716\n"
     ]
    }
   ],
   "source": [
    "# 명차추출 dict 무슨 값이냐? \n",
    "# sklearn.countervectorizer tokenizer에 들어갈 값을 만들기위해.........\n",
    "# custom_tokenize에서 하는일은?\n",
    "\n",
    "import pickle\n",
    "with open('../NPL/code/day6/2016-10-24-extract_noun_dict.pkl','rb') as f:\n",
    "    noun_dict = pickle.load(f)\n",
    "\n",
    "for i, dict in enumerate(noun_dict):\n",
    "    if i > 10 : break\n",
    "    print(dict, noun_dict[dict])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom tokenizer \n",
    "# countvectorizer 인자로 들어가는 tokenizer를 만드는 이유?\n",
    "# 명사만 추출해 주는것 같음.. 아무래도 tokenize니까... \n",
    "# 이 함수를 사용하는건 좀 더 알아보고 해야겠다.. \n",
    "# list comprehension 방식을 리딩할수 있어야한다...\n",
    "\n",
    "def custom_tokenize(doc):\n",
    "    def parse_noun(token):\n",
    "        for e in reversed(range(1, len(token)+1)):\n",
    "            subword = token[:e]\n",
    "            if subword in noun_dict:\n",
    "                return subword\n",
    "        return ''\n",
    "    nouns = [parse_noun(token) for token in doc.split()]\n",
    "    nouns = [word for word in nouns if word]\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['테스트', '테스트', '테스트']\n"
     ]
    }
   ],
   "source": [
    "# twitter 방식.. 이미 학습된 내용이며 평균이상의 결과물을 보이나 \n",
    "# 도메인에따라 학습된 내용이 아닌 단어인 경우 결과가 좋지 않음.. \n",
    "# 최선의 방법은 도메인에 따라 학습된 것을 바탕으로 custom_tokenize를 돌려야 함..\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "#         pos = ['{}/{}'.format(word,tag) for word, tag in pos if tag=='Noun']\n",
    "        pos = ['{}'.format(word) for word,tag in pos if tag=='Noun' and len(word)>1]\n",
    "        return pos\n",
    "\n",
    "tw_tokenizer = MyTokenizer(Twitter())\n",
    "print(tw_tokenizer('테스트는 테스트로 테스트한다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(tokenizer = tw_tokenizer)\n",
    "x = vectorizer.fit_transform(corpus_doc)\n",
    "idx2vocab = [vocab  for vocab, idx in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26368, 52344)\n",
      "가가\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(idx2vocab[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA는 sklean과 gensim에 구현되어있는것 같은데 사용하는 data 형식이 다르다...\n",
    "sparse / dense matrix에 대한 이해필요..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "3.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mediance_ssh\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(scipy.__version__)\n",
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#doc= 0:  [] ...\n",
      "#doc= 1:  [(9941, 1), (4602, 2), (3189, 1), (35025, 1), (1953, 3), (43868, 2), (9611, 1), (15603, 1), (34368, 1), (21675, 1)] ...\n",
      "#doc= 2:  [(597, 1), (51304, 1), (9530, 1), (50513, 2), (1315, 1), (52039, 1), (51502, 1), (19214, 1), (1738, 1), (39423, 1)] ...\n",
      "#doc= 3:  [(37890, 1), (21760, 1), (3965, 1), (30334, 1), (19333, 1), (35542, 1), (21062, 1), (20203, 1), (10601, 1), (8074, 1)] ...\n"
     ]
    }
   ],
   "source": [
    "corpus_sparse = gensim.matutils.Sparse2Corpus(x, documents_columns=False)\n",
    "\n",
    "for i, doc in enumerate(corpus_sparse):\n",
    "    if i > 3: break\n",
    "    print('#doc= %d: ' % i, doc[:10], '...' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26368\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary formats 야이.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "row, col = x.nonzero()\n",
    "data = [1]*len(row)\n",
    "x_boolean = csr_matrix((data, (row, col)))\n",
    "df = x_boolean.sum(axis=0)\n",
    "\n",
    "n_doc = x.shape[0]\n",
    "word2index = vectorizer.vocabulary_\n",
    "df = df.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary 작성...\n",
    "with open(dictionary_fname, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d\\n' % n_doc)\n",
    "    for word, idx in word2index.items():\n",
    "        f.write('%d\\t%s\\t%d\\n' % (idx, word, df[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary.load_from_text(dictionary_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(44692, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.doc2bow(['코코모']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "if TRAIN_LDA:\n",
    "    ldamodel = LdaModel(corpus=corpus_sparse, num_topics=20, id2word=dictionary)\n",
    "    import pickle\n",
    "    with open(ldamodel_fname, 'wb') as f:\n",
    "        pickle.dump(ldamodel, f)\n",
    "else: \n",
    "    import pickle\n",
    "    with open(ldamodel_fname, 'rb') as f:\n",
    "        ldamodel = pickle.load(f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012*\"사고\" + 0.010*\"제주\" + 0.009*\"일간스포츠\" + 0.008*\"현지\" + 0.007*\"시간\"\n",
      "[(21642, 0.012161469), (1258, 0.0116676865), (33830, 0.009331483), (5010, 0.0089304205), (21403, 0.008419862), (35942, 0.007967435), (5042, 0.007921392), (28905, 0.007892781), (47533, 0.0077366466), (13714, 0.0076230825)]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(10,topn=5))\n",
    "print(ldamodel.get_topic_terms(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_topic_words(topic_str):\n",
    "    return [col.split('*\"')[1][:-1] for col in topic_str.split(' + ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#topic= 0: ['아파트', '지역', '단지', '가구', '대출']\n",
      "#topic= 1: ['상승', '거래', '은행', '금융', '삼성']\n",
      "#topic= 2: ['경찰', '경우', '치료', '병원', '환자']\n",
      "#topic= 3: ['문화', '나리', '예술', '작품', '우리']\n",
      "#topic= 4: ['대표', '의원', '회의', '기자', '북한']\n",
      "\n",
      "#topic= 5: ['삼성', '갤럭시', '시장', '전자', '노트']\n",
      "#topic= 6: ['교육', '사회', '도서관', '학생', '클린턴']\n",
      "#topic= 7: ['기자', '공개', '영화', '한국', '앨범']\n",
      "#topic= 8: ['트와이스', '기자', '걸그룹', '서울', '쇼케이스']\n",
      "#topic= 9: ['스포츠서울', '제보', '사진', '기자', '뉴스']\n",
      "\n",
      "#topic= 10: ['사고', '제주', '일간스포츠', '현지', '시간']\n",
      "#topic= 11: ['게임', '지역', '지진', '트레인', '경기도']\n",
      "#topic= 12: ['고객', '네이버', '한남동', '서비스', '통해']\n",
      "#topic= 13: ['독도', '경제', '년생', '시선', '치매']\n",
      "#topic= 14: ['뉴스', '무단', '기자', '전재', '서울']\n",
      "\n",
      "#topic= 15: ['재단', '수사', '검찰', '최씨', '일보']\n",
      "#topic= 16: ['사업', '경제', '지원', '기업', '산업']\n",
      "#topic= 17: ['방송', '자신', '모습', '배우', '드라마']\n",
      "#topic= 18: ['서울', '기자', '뉴스', '금지', '배포']\n",
      "#topic= 19: ['개헌', '대통령', '논의', '국회', '국민']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('#topic= %d:' % i, parse_topic_words( ldamodel.print_topic(i, topn=5)))\n",
    "    if i % 5 == 4: print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 52344)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.expElogbeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = ldamodel.expElogbeta.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n"
     ]
    }
   ],
   "source": [
    "topn =20\n",
    "important_terms = beta.argsort(axis=1)[:,-topn:]\n",
    "print(important_terms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "topic #0: 아파트 지역 단지 가구 대출 주민 시설 주택 부동산 분양 시장 만원 세종 공원 강남 건설 예정 할머니 서울 시민\n",
      "topic #1: 상승 거래 은행 금융 삼성 일보 금지 억원 평균 매수 주가 만원 배포 무단 기자 대비 전재 최근 의견 현재\n",
      "topic #2: 경찰 경우 치료 병원 환자 교수 기자 왕소 때문 피부 사람 인체 결과 금지 대한 임금 배포 수술 영장 전재\n",
      "topic #3: 문화 나리 예술 작품 우리 공연 모습 마을 가을 해수 황제 거리 사람 이야기 체험 달빛 무단 축제 기자 공개\n",
      "topic #4: 대표 의원 회의 기자 북한 민주당 장관 위원회 새누리당 수석 무단 입장 대해 예산 대통령 청와대 국회 위원장 최순실 배포\n",
      "\n",
      "topic #5: 삼성 갤럭시 시장 전자 노트 투자 제품 출시 가격 억원 중국 브랜드 매출 업체 국내 기자 고객 분기 기업 판매\n",
      "topic #6: 교육 사회 도서관 학생 클린턴 트럼프 사람 시간 프로그램 여성 문화 후보 생활 장애인 지원 창업 위해 무단 한국 청년\n",
      "topic #7: 기자 공개 영화 한국 앨범 아시아 지난 통해 멤버 스타 기사 사진 음악 매력 무대 가수 활동 배포 세계 금지\n",
      "topic #8: 트와이스 기자 걸그룹 서울 쇼케이스 블루 미니앨범 스퀘어 음원 신곡 오후 금지 무단 전재 이번 차트 배포 타이틀곡 행사 사랑\n",
      "topic #9: 스포츠서울 제보 사진 기자 뉴스 인스타그램 카카오 기증 서울 전재 무단 마약 배포 박수진 임신 금지 소속사 언론 롯데 박규리\n",
      "\n",
      "topic #10: 사고 제주 일간스포츠 현지 시간 배포 기자 무단 미국 금지 버스 중국 전재 병원 닥터 오후 부상 노조 제주도 인천\n",
      "topic #11: 게임 지역 지진 트레인 경기도 기자 체험 경기 배포 닥터 수원 청소년 통해 관광 네팔 한국 일본 금지 이번 규모\n",
      "topic #12: 고객 네이버 한남동 서비스 통해 기자 이벤트 기술 무단 제품 금지 커피 제공 만원 티켓 전재 르노 배포 쇼핑 뉴스\n",
      "topic #13: 독도 경제 년생 시선 치매 무단 금지 기자 일본 건강 배포 사진 저작권 전재 유승우 약물 치료 트로트 뉴스 한국\n",
      "topic #14: 뉴스 무단 기자 전재 서울 배포 금지 국회 코리아 연설 대통령 시정 박근혜 예산안 오전 여의도 본회의 경제 년도 관련\n",
      "\n",
      "topic #15: 재단 수사 검찰 최씨 일보 기자 의혹 혐의 스포츠 사실 금지 지난 회장 롯데 배포 무단 억원 보험 미르 교수\n",
      "topic #16: 사업 경제 지원 기업 산업 한국 정부 위해 계획 기술 대한 내년 문화 연구 예산 개발 금융 통해 지역 세계\n",
      "topic #17: 방송 자신 모습 배우 드라마 기자 사람 공개 데뷔 사진 라며 대한 웃음 한편 시청자 배포 에서 고호 지난 연기\n",
      "topic #18: 서울 기자 뉴스 금지 배포 전재 무단 오후 난민 저작권 오전 한글 철거 여의도 시민 종로구 한강 사진 노컷뉴스 프랑스\n",
      "topic #19: 개헌 대통령 논의 국회 국민 헌법 임기 대표 정부 정치 문제 지금 박근혜 입장 대선 연설 기자 개정 청와대 경제\n"
     ]
    }
   ],
   "source": [
    "for topic_idx in range(beta.shape[0]):\n",
    "    if topic_idx % 5 == 0:\n",
    "        print()\n",
    "    term_indices = important_terms[topic_idx,:].reshape(-1)\n",
    "    terms = reversed([idx2vocab[idx] for idx in term_indices])\n",
    "    print('topic #{}: {}'.format(topic_idx, ' '.join(terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test end..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim의 topic modeling을 이용하기 위해서 전처리와 sparse matrix 변환을 해봄\n",
    "전처리에서 custom tokenizer 사용은 품질관리에 큰 영향을 주는것 같음... postagging이 중요한거라면... 이것도 따로 적용한걸 써야함.. \n",
    "sparse matrix는 gensim에서 정해저있는 방식으로 값을 넘겨야해서.. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
