{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 gensim 튜토리얼과 https://wikidocs.net/30708 내용을 토대로 sklearn에 있는 LDA를 적용해본다.. \n",
    "이번엔 hashtag파일을 가지고 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_fname = './data/hashtag_test.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "팔 명지대학교 그린나래 기획봉사 농촌활동 내\n",
      "\n",
      "아역모델 작시 zaxy 작시슈즈 엠드림즈 브랜드 ootd 패피 신발 광고모델 김하은\n",
      "\n",
      "보이차 아지트샵 아지트샵조히\n",
      "\n",
      "무한체력 엄마는 너가 조용하면 좀 무섭다\n",
      "\n",
      "감사해 에보나이트 그래도빈손아니니까\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(corpus_fname, encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "corpus_doc = DoublespaceLineCorpus(corpus_fname, iter_sent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "팔 명지대학교 그린나래 기획봉사 농촌활동 내\n",
      "\n",
      "아역모델 작시 zaxy 작시슈즈 엠드림즈 브랜드 ootd 패피 신발 광고모델 김하은\n",
      "\n",
      "보이차 아지트샵 아지트샵조히\n",
      "\n",
      "무한체력 엄마는 너가 조용하면 좀 무섭다\n",
      "\n",
      "감사해 에보나이트 그래도빈손아니니까\n",
      "\n",
      "시바스타그램 kyoto shibainu kaji uruwasiki shibakaji shiba 시바견 시바 시바이누 시바개 견스타그램 독스타그램 도그스타그램\n",
      "\n",
      "스타패션 imjinah nana 임진아 나나 킬잇 드라마패션 아보아보 avouavou kstyle killit imjinahstyle nanastyle kdramastyle kdrama_fashion kfashion 연예인패션 연예인착용 연예인스타일 임진아패션 임진아스타일 임진아원피스 나나패션 나나원피스 패션정보 드라마패션그램\n",
      "\n",
      "원본 셀카\n",
      "\n",
      "양세형\n",
      "\n",
      "울산대\n",
      "\n",
      "딸램그램 아보카도버거굿 메뉴판정독후 감자튀김만열심히\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(corpus_doc):\n",
    "    if i> 10 : break\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603108\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tag 데이터는 기본적으로 tokenizing이 되어있는 명사 형태인경우가 많아 그대로 사용해보자.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features= 3000)\n",
    "X = vectorizer.fit_transform(corpus_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(603108, 3000)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model=LatentDirichletAllocation(n_components=50, learning_method='online', random_state=777, max_iter=1)\n",
    "lda_top = lda_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 1: ['자연눈썹', '나이키', '선물추천', '눈썹문신', '집들이선물', '스트릿패션', '남자눈썹문신', '반영구화장', '공구', '스트릿', '추억팔이', '광주꽃집', '강남반영구', '엠보눈썹', '포토그래퍼', '훈스타그램', '대구반영구', '반영구수강', '눈썹반영구', '대구속눈썹']\n",
      "topic 2: ['카페', '광주', 'lfl', '카페투어', '추천', '카페인테리어', '염색', '생일선물', '청주맛집', '파주', '예비맘', '휴식', '황금돼지띠맘', '임산부', '상무지구', '동명동', '충장로', '연산동', '부산속눈썹', '음스타그램']\n",
      "topic 3: ['가족여행', '워킹맘', '생일파티', '남자눈썹', '대출', '팔찌', '예쁜카페', '벌써', '강화도', '시바견', '더베이101', '엄마선물', '마린시티', '인천공항', '숏컷', '수원맘', '제주흑돼지맛집', '카페라떼', '핸드드립', '행복스타그램']\n",
      "topic 4: ['일상', '사진', '일상그램', 'beauty', '결혼', 'photography', 'photo', '남매맘', '스냅', '4월', '원주', '천안', '저녁메뉴', '주부', '인생샷', '고3', '니콘', '행사', '부케', '전신샷']\n",
      "topic 5: ['ad', '어버이날선물', '어버이날', '이벤트', '원데이클래스', '카네이션', '부모님선물', '스승의날선물', '올리브영', '결혼기념일', '떡케이크', '앙금플라워', '천안맘', '다이어리', '일기', '앙금플라워떡케이크', '골스타그램', '플라워케이크', '육아용품', '분위기깡패']\n",
      "topic 6: ['내사랑', '발렌시아가', '내일', '기념일선물', '가죽공예', '남매', '버버리', '공스타', '레이어드펌', '보고싶다', '남자머리', 'hair', '강남미용실', '로렉스', '허쉬컷', '데일리백', '커플템', '오빠', '공주님', '가죽공방']\n",
      "topic 7: ['영화', '책스타그램', '메이크업', '독서', '인싸', '공감', '유튜브', '책추천', '좋은글', '글귀', '글스타그램', '글귀스타그램', '아디다스', '좋아요환영', 'bigbang', '오늘하루', '영화스타그램', '에세이', '도전', '감성글']\n",
      "topic 8: ['먹스타그램', '맛스타그램', '먹방', 'jmt', '존맛탱', '맛있다그램', '가로수길', '맛집', '야식', '강남역맛집', '먹부림', '데이트코스', '춘천', '존맛', '먹방스타그램', '주말스타그램', 'kdrama', 'korean', '브런치', '일상']\n",
      "topic 9: ['첫줄', '사랑해', '딸스타그램', '아들', '오늘', '애스타그램', '고마워', '세젤귀', '딸바보', '주말일상', '필라테스', '아기', '세젤예', '우리집', '베이비그램', '키즈스타그램', '육아', '엄마', '부산맘', '베이비']\n",
      "topic 10: ['멍스타그램', '반려견', '강아지', '댕댕이', '개스타그램', 'dog', 'dogstagram', '펫스타그램', '독스타그램', '말티즈', '가족스타그램', '견스타그램', 'instadog', 'puppy', '비숑', '멍멍이', '댕댕이그램', '반려동물', '인스타독', 'pet']\n",
      "topic 11: ['daily', 'f4f', 'fff', 'instagood', 'follow', 'fashion', '인스타그램', 'likeforlikes', 'instagram', 'instadaily', '토요일', 'instalike', '일상', '한강', '운동스타그램', '배우', 'likeforfollow', 'followme', 'japan', 'model']\n",
      "topic 12: ['뷰티스타그램', '뷰티', '창원', '코덕', '디저트', '뷰스타그램', '키즈모델', '분위기', '뷰티그램', 'exo', '홈케어', '마산', '리그램이벤트', '맛스타', '뷰티템', 'makeup', '아역배우', '그래도', '혼술', '한우']\n",
      "topic 13: ['일요일', '오늘도', '등산', '금요일', '배고파', '가즈아', '예쁘다', '공팔리터', '여름준비', '투데이', '그냥', '덕천맛집', '산스타그램', '08l', '인스타일상', '유료광고', '등산스타그램', '핑크', '이쁘다', '치어리더']\n",
      "topic 14: ['애드펌킨', '오늘은', '인싸템', '인테리어소품', '엑소', '웨딩밴드', '커플링', '친구들', '웨딩링', 'beautiful', 'dessert', '아이언맨', '캔들', '포항맛집', 'superjunior', '딸기', '광화문맛집', '캔들공방', '빈티지소품', '향수']\n",
      "topic 15: ['고양이', '비오는날', '냥스타그램', '선물', '집밥', 'cat', '캣스타그램', '포메라니안', 'catstagram', '요리스타그램', '온더테이블', '홈쿡', '반려묘', '집밥스타그램', '요리', '한식', '코엑스', '쿡스타그램', '길냥이', '음악']\n",
      "topic 16: ['일상스타그램', '데일리', '카페스타그램', '인친환영', '맞팔환영', '일상', '소통환영', '소통해요', '맞팔해요', '소통스타그램', '사진스타그램', '선팔하면맞팔가요', '커피스타그램', '데일리코디', '남매스타그램', '맞팔선팔', '데일리패션', '순천', '육아소통환영', '소통']\n",
      "topic 17: ['속눈썹연장', '이달의아트', '속눈썹펌', '부평', '최고', '부천', '감동', '뷰러펌', '축하해', '인천네일', '반영구', '단발펌', '놀이터', '왁싱', '퍼플유', '울산눈썹문신', '마지막', '세부', '목동', '강남속눈썹']\n",
      "topic 18: ['안산', '볼링', '돌잔치', '봄코디', '호캉스', '명품', '백일상대여', '백일상', '미용실', '키즈룩', '돌선물', '대전맘', '김밥', '조카선물', '춘천맛집', '동탄맘', '돌상', '감사해요', '돌상대여', '돌촬영']\n",
      "topic 19: ['서울', '홍대', '강남', '이태원', '불금', '연남동', '베트남', '떡볶이', '거울샷', '아이폰', '아이폰케이스', '단발', '하루', '둔산동', '압구정', '핫플레이스', '잠실', '이태원맛집', '건대', '신림']\n",
      "topic 20: ['대구', 'kpop', '친구', '가족', '대구맘', 'music', '취미생활', '꿀잼', '동탄', '강다니엘', 'kangdaniel', '부산꽃배달', '칠곡', '서면꽃집', 'idol', '인계동', '마마무', '콤보눈썹', '해운대꽃집', '셀프네일']\n",
      "topic 21: ['월요일', '헬요일', '스타벅스', '공스타그램', '시험', '시험기간', '공부', '동대문', '진짜', '피곤', '감성카페', '공부스타그램', '구름', '쉬는날', '힘내자', 'starbucks', '공시생', '회사원', '행복한', '스벅']\n",
      "topic 22: ['korea', 'seoul', '푸들', 'tattoo', '어제', '양평', '광주카페', '미니타투', '30대', '문의', '감성타투', '라이프스타일', 'c컬펌', '타투도안', '신용대출', '허벌라이프코치', '봄꽃', '라인타투', '쌩얼', '꽃타투']\n",
      "topic 23: ['가정의달', '선팔환영', '두산베어스', '삼교리동치미막국수', '디엠', '얼짱', '전시회', '대만', '가로수길맛집', '학생', '익산', '투잡', '케이크토퍼', '군산', 'parkjihoon', '박지훈', '팔로우환영', '여행토퍼', '거제도맛집', '필수템']\n",
      "topic 24: ['주말', '어벤져스엔드게임', '어벤져스', '꽃다발', '꽃스타그램', 'flower', '어린이날', '플라워레슨', '감성사진', '촬영', '꽃바구니', '플로리스트', '타투', 'flowers', '용돈박스', '엔드게임', '마블', '꽃집', '플라워클래스', 'florist']\n",
      "topic 25: ['대전', '루이비통', '다이어트식단', '디자인', '라이딩', 'design', '사랑합니다', '허벌라이프', '자덕', 'cycling', '리모델링', '웰시코기', 'youtube', '노을', '지창욱', '호주', '제주도스냅', 'jichangwook', '식단조절', '백패커']\n",
      "topic 26: ['bts', '방탄소년단', '세븐틴', 'army', 'seventeen', 'carat', '먹스타', '패피', 'wannaone', 'dance', '댄스', '여의도맛집', '럽스타', '워너원', '흑백사진', '동래', '김태형', 'jungkook', 'jimin', '성공']\n",
      "topic 27: ['일러스트', 'art', 'drawing', 'illustration', '화장품', '드로잉', '굿밤', 'artwork', 'artist', '고기', '새벽', '그림스타그램', '치와와', 'illust', '군인', '손그림', '파리', '수채화', 'watercolor', '예술']\n",
      "topic 28: ['다이어트', 'repost', '데이트', '운동하는여자', '운동하는남자', '헬스', '강남맛집', '다이어터', '셀카그램', '구미', '등원룩', '미용인', '트와이스', '좋아', '미용', '다이어트그램', '붙임머리', '기분전환', '운동선수', 'nike']\n",
      "topic 29: ['부산', '해운대', '부산맛집', '서면', '부산여행', '광안리', '부산호빠', '해운대호빠', '부산호스트바', '공연', '부산카페', '오랜만에', '남포동', '서면맛집', '뮤지컬', '대구카페', '광안리맛집', '해운대호스트', '서면호스트바', '경주맛집']\n",
      "topic 30: ['예신', '결혼준비', '모델', '원피스', '인물사진', '스냅사진', '아동복', 'cute', '하객룩', '대구핫플', 'wedding', '빈티지', '진주', '오늘의훈녀', '웨딩드레스', '셀프웨딩', '예비신부', '웨딩스냅', 'portrait', '스냅촬영']\n",
      "topic 31: ['바다', '사지말고입양하세요', '신상', '주문제작', '펍벤', '레플리카', '답례품', '좋아요늘리기', '홍콩', '쇼핑몰', '여성의류', '남자옷', '데일리룩코디', '가방', '속눈썹', '모자', '도매', '인기게시물', '기업출강', '뚱카롱']\n",
      "topic 32: ['감사합니다', '귀요미', '드라마', '일산맛집', '내새끼', '베이비스타그램', '팔로우미', '봄나들이', '비숑프리제', 'baby', '가평', '대학로', '문화생활', '유아식', 'asmr', '인스타마케팅', '베이비룩', 'sbs', '1박2일', '커피한잔']\n",
      "topic 33: ['럽스타그램', '인스타', '소확행', '결혼식', '사랑', '커플', '신혼부부', '감사', '의정부', '신혼', '커플룩', '커플스타그램', '데이트룩', '야경', '남자친구', '파스타', '대박', '안양', '하트', '디저트카페']\n",
      "topic 34: ['인테리어', '그림', '삼겹살', '주부스타그램', '5월', '취향저격', '인테리어디자인', '창업', '울산맘', '가구', '스티커', '주말그램', '로고', '실시간', '스몰웨딩', '식물', '케이터링', '오픈', '이자카야', '하우스웨딩']\n",
      "topic 35: ['아지트샵', '어린이날선물', '화이팅', '부부스타그램', '일상기록', '울산맛집', '안녕', '강남역', '후기톡', '이제', '아지트홈', '라이프톡', '부업', '집사그램', '역시', '다시', '직딩', '일상사진', '카네이션꽃다발', '아지트샵멘토']\n",
      "topic 36: ['요가', '레이어드컷', '빌드펌', '본식스냅', '부산꽃집', '축제', '벚꽃', '프로포즈', '스시', '혜화', '연극', '노원', '베이킹클래스', '대전눈썹문신', '담백한브랜딩', '준오헤어', '평일', '유산균', '미용실추천', '장미']\n",
      "topic 37: ['핸드메이드', 'handmade', '케이크', '김해', '인천맛집', '헬스타그램', '안산맛집', '이천', '대부도맛집', '연산동맛집', '대부도', 'interior', '호텔', '김해맛집', '석고방향제', '치즈', '강아지분양', '천연비누', 'cake', '눈바디']\n",
      "topic 38: ['육아스타그램', '육아맘', '육아', '육아소통', '맘스타그램', '아들스타그램', '아들맘', '도치맘', '주말나들이', '인스타베이비', '육아일기', '샤넬', '딸맘', '육아템', '일상', '젊줌마', '육아일상', '여름', '워킹맘', '아기스타그램']\n",
      "topic 39: ['제주도', '제주', '네일아트', '젤네일', '봄네일', '제주여행', '네일', '제주맛집', '나들이', '포항', '제주도여행', '네일스타그램', '산책', '대학생', '제주도맛집', 'nail', '웨딩네일', 'nailart', 'jeju', '화려한네일']\n",
      "topic 40: ['울산', '운동', '구찌', '강릉', '해운대호스트바', '추억', '훈남', '맥주', '귀걸이', '러닝', '잠실맛집', '마라톤', '겹벚꽃', '런스타그램', '여자친구선물', '롯데월드', '후쿠오카', 'running', '브라이덜샤워', '달리기']\n",
      "topic 41: ['데일리룩', 'ootd', '오오티디', '직장인', '패션', 'dailylook', '생일', '옷스타그램', '퇴근', '일산', '월요병', '스타일', '출근', '데일리', '직장인스타그램', '일상', '휴무', 'style', '셀피그램', 'daily']\n",
      "topic 42: ['힐링', 'love', '인천', '쇼핑', '주말데이트', '부부', 'kbeauty', '불토', '수원맛집', '날씨좋다', 'happy', '스쿠버다이빙', '오키나와', 'spring', '서핑', '카메라', '스킨케어', '마스크팩', '가수', '힐링여행']\n",
      "topic 43: ['맞팔', '좋아요', '소통', '선팔', '셀카', '일상', '셀스타그램', '좋아요반사', '셀피', '팔로우', '데일리', '선팔하면맞팔', 'selfie', '인친', '좋반', 'daily', '얼스타그램', 'l4l', '선팔맞팔', 'ootd']\n",
      "topic 44: ['여수', '피부관리', '강남호빠', '훈녀', '수요비', '선릉호빠', '자체제작', '부천호빠', '신림호빠', '부평호빠', '레터링케이크', '여행룩', '이태원호빠', '루이비통가방', '영등포호빠', '10개월아기', '낭만포차', '흔녀', '화곡호빠', '신촌호빠']\n",
      "topic 45: ['여행', '여행스타그램', '여행에미치다', 'travel', '일본', '캠핑', '북스타그램', '일본여행', '풍경', '강릉여행', '경주여행', '강원도', '유럽여행', '백패킹', 'camping', 'landscape', '다낭', 'trip', 'photooftheday', '여행그램']\n",
      "topic 46: ['맛집', '술스타그램', 'food', '저녁', '맛스타그램', 'instafood', '점심', '맛있다', 'foodstagram', '냠냠', '푸드스타그램', '치킨', '맛집추천', '간식', 'koreanfood', '맛집스타그램', 'foodie', '건강', '와인', '축구']\n",
      "topic 47: ['경주', '전주', '날씨', '너무', '굿모닝', '시작', '동영상', '셀프인테리어', '대전카페', '전주맛집', '튤립', '자유부인', '전주한옥마을', '운스타그램', '전주여행', '라떼', 'instafashion', 'hobby', '감기', '육아정보']\n",
      "topic 48: ['집스타그램', '광주맛집', '미세먼지', '홈스타그램', '리뉴메디', '가족사진', '집꾸미기', '성공적', '취미미술', '홈인테리어', '부산반영구', '홈스타일링', '밥스타그램', '신혼스타그램', '거실인테리어', '신혼집', '새댁스타그램', '목포', '상무지구맛집', '나주']\n",
      "topic 49: ['취미', '대구맛집', '첫줄안녕', '커피', 'cafe', '감성', '동성로', '웨딩', '드라이브', '대구', '홈카페', '캘리그라피', '웨딩촬영', 'coffee', '차스타그램', '동성로맛집', '동성로술집', '야구', '일상', '카스타그램']\n",
      "topic 50: ['행복', '청주', '골프', '귀여워', '골프스타그램', 'golf', '사랑둥이', '사랑해요', '휴가', '주말여행', 'hbd', '자전거', '낚시', '배드민턴', '마무리', '골프스윙', '제발', '골프웨어', '취미스타그램', '골프레슨']\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "def get_topics(components, feature_names, n=20):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print('topic %d:' % (idx+1), [(feature_names[i]) for i in topic.argsort()[:-n - 1:-1]])\n",
    "\n",
    "get_topics(lda_model.components_, terms)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#doc= 0:  [] ...\n",
      "#doc= 1:  [(1404, 0.509601690455671), (324, 0.29009377801143543), (2753, 0.4449523235358275), (1731, 0.47481479290994805), (633, 0.48241067490026185)] ...\n",
      "#doc= 2:  [(1816, 1.0)] ...\n",
      "#doc= 3:  [] ...\n"
     ]
    }
   ],
   "source": [
    "# 이 블럭은 pyLDAvis 에서 필요한 부분\n",
    "import gensim\n",
    "corpus_sparse = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "for i, doc in enumerate(corpus_sparse):\n",
    "    if i > 3: break\n",
    "    print('#doc= %d: ' % i, doc[:10], '...' )\n",
    "\n",
    "# dictionary \n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "row, col = X.nonzero()\n",
    "data = [1]*len(row)\n",
    "x_boolean = csr_matrix((data, (row, col)))\n",
    "df = x_boolean.sum(axis=0)\n",
    "\n",
    "n_doc = X.shape[0]\n",
    "word2index = vectorizer.vocabulary_\n",
    "df = df.tolist()[0]\n",
    "\n",
    "# dictionary는 무엇인가..?\n",
    "dictionary_fname = './data/hashtag_test_LDA_sklearn.dictionary'\n",
    "with open(dictionary_fname, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d\\n' % n_doc)\n",
    "    for word, idx in word2index.items():\n",
    "        f.write('%d\\t%s\\t%d\\n' % (idx, word, df[idx]))\n",
    "        \n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary.load_from_text(dictionary_fname)\n",
    "print(dictionary.doc2vow(['일상']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "print(pyLDAvis.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'token2id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f1b3e6d12feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgensimvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprepared_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensimvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_sparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\mediance_ssh\\Anaconda3\\lib\\site-packages\\pyLDAvis\\gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\mediance_ssh\\Anaconda3\\lib\\site-packages\\pyLDAvis\\gensim.py\u001b[0m in \u001b[0;36m_extract_data\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparse2Corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_csc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m    \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m    \u001b[1;31m# TODO: add the hyperparam to smooth it out? no beta in online LDA impl.. hmm..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m    \u001b[1;31m# for now, I'll just make sure we don't ever get zeros...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'token2id'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim as gensimvis\n",
    "prepared_data = gensimvis.prepare(lda_model, corpus_sparse, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test end..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn으로 tfdifvectorizer 사용해서 LDA 돌리는게 더 간편하고 직관적인것 같다?\n",
    "토크나이저 돌리기 위해서는 몇가지 처리를 좀더 해야하긴 함\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
